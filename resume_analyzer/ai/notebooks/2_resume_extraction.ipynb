{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective: Extract structured information from resumes (.pdf or .docx)\n",
    "\n",
    "Tools: spaCy for NER, regex for pattern matching, and file-specific parsers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from docx import Document\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from transformers import pipeline\n",
    "import re\n",
    "import ipywidgets as widgets\n",
    "from datetime import datetime\n",
    "from IPython.display import display, HTML,Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Resume File Text Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Resume File Text Extraction (pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    \"\"\"\n",
    "    Extracts and formats text from a PDF into a resume-friendly structure.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned and well-formatted text suitable for parsing.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with fitz.open(file_path) as doc:\n",
    "            raw_text = \"\\n\".join(\n",
    "                page.get_text(\"text\").strip()\n",
    "                for page in doc\n",
    "                if page.get_text(\"text\").strip()\n",
    "            )\n",
    "\n",
    "        if not raw_text:\n",
    "            raise ValueError(\"PDF contains no extractable text (possibly scanned image).\")\n",
    "\n",
    "        # Clean and enhance layout\n",
    "        text = raw_text\n",
    "\n",
    "        # Normalize line endings and collapse excessive newlines\n",
    "        text = re.sub(r'\\n{2,}', '\\n\\n', text)\n",
    "        lines = [line.strip() for line in text.splitlines()]\n",
    "        text = \"\\n\".join(lines)\n",
    "\n",
    "        # Normalize bullets\n",
    "        text = re.sub(r'^\\s*[•·]', '-', text, flags=re.MULTILINE)\n",
    "\n",
    "        # Normalize section headers (Education, Skills, etc.)\n",
    "        section_keywords = [\n",
    "            \"Summary\",\"Professional Summary\",\"Career Objective\" \"Skills\", \"Experience\", \"Education\", \"Certifications\",\n",
    "            \"Projects\", \"Languages\", \"Interests\", \"Volunteer\", \"References\"\n",
    "        ]\n",
    "        for keyword in section_keywords:\n",
    "            text = re.sub(rf\"(?<!\\w){keyword.upper()}(?!\\w)\", f\"{keyword}:\", text)\n",
    "\n",
    "        return text\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"PDF extraction failed for '{file_path}': {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Resume File Text Extraction (docx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_text_from_docx(file_path):\n",
    "    \"\"\"\n",
    "    Extracts text content from a DOCX file (paragraphs only).\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the .docx file.\n",
    "\n",
    "    Returns:\n",
    "        str: Cleaned, joined paragraph text from the document.\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError: If the file cannot be read or is empty.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        doc = Document(file_path)\n",
    "\n",
    "        # Extract and clean paragraph text\n",
    "        text_parts = [p.text.strip() for p in doc.paragraphs if p.text.strip()]\n",
    "        combined_text = \"\\n\".join(text_parts)\n",
    "\n",
    "        if not combined_text:\n",
    "            raise ValueError(\"DOCX file contains no extractable paragraph text.\")\n",
    "\n",
    "        return combined_text\n",
    "\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"DOCX extraction failed for '{file_path}': {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Resume File Text Extraction (docx or pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_resume_text(file_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a resume file (PDF or DOCX).\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the uploaded resume file.\n",
    "\n",
    "    Returns:\n",
    "        str: Extracted plain text from the file.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the file extension is not supported.\n",
    "        RuntimeError: If extraction from a supported file fails.\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(file_path):\n",
    "        raise FileNotFoundError(f\"The file does not exist: {file_path}\")\n",
    "\n",
    "    ext = os.path.splitext(file_path)[1].lower()\n",
    "\n",
    "    if ext == \".pdf\":\n",
    "        try:\n",
    "            return extract_text_from_pdf(file_path)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to extract text from PDF: {e}\")\n",
    "\n",
    "    elif ext == \".docx\":\n",
    "        try:\n",
    "            return extract_text_from_docx(file_path)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to extract text from DOCX: {e}\")\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Only PDF and DOCX files are allowed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Clean Text (Minimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    return re.sub(r'\\s+', ' ', text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Extract Entities with spaCy (Name, Org, Location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yousuf\\anaconda3\\envs\\analyzer\\lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Yousuf\\.cache\\huggingface\\hub\\models--dbmdz--bert-large-cased-finetuned-conll03-english. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Error while downloading from https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english/resolve/main/model.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n",
      "c:\\Users\\Yousuf\\anaconda3\\envs\\analyzer\\lib\\site-packages\\transformers\\pipelines\\token_classification.py:170: UserWarning: `grouped_entities` is deprecated and will be removed in version v5.0.0, defaulted to `aggregation_strategy=\"simple\"` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ner_pipeline = pipeline(\n",
    "    \"ner\",\n",
    "    model=\"dbmdz/bert-large-cased-finetuned-conll03-english\",\n",
    "    grouped_entities=True,\n",
    "    framework=\"pt\",\n",
    "    device=0\n",
    ")\n",
    "\n",
    "def extract_transformer_entities(\n",
    "    text: str,\n",
    "    ner_model=None,\n",
    "    return_first_only=True,\n",
    "    label_map=None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Extract entities (Name, Organization, Location) using a Hugging Face NER model.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text from resume or document.\n",
    "        ner_model: Optional. A Hugging Face transformers NER pipeline. If None, it will be created.\n",
    "        return_first_only (bool): If True, return only the first match per entity type.\n",
    "        label_map (dict): Optional. Mapping of entity types to model-specific NER labels.\n",
    "\n",
    "    Returns:\n",
    "        dict: {\n",
    "            \"Name\": str or List[str],\n",
    "            \"Organization\": str or List[str],\n",
    "            \"Location\": str or List[str]\n",
    "        }\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return {\"Name\": None, \"Organization\": None, \"Location\": None}\n",
    "\n",
    "    if ner_model is None:\n",
    "        ner_model = pipeline(\"ner\", grouped_entities=True)\n",
    "\n",
    "    # Default entity label mappings\n",
    "    label_map = label_map or {\n",
    "        \"Name\": [\"PER\", \"PERSON\"],\n",
    "        \"Organization\": [\"ORG\"],\n",
    "        \"Location\": [\"LOC\", \"GPE\"]\n",
    "    }\n",
    "\n",
    "    raw_entities = ner_model(text)\n",
    "    result = {\"Name\": [], \"Organization\": [], \"Location\": []}\n",
    "\n",
    "    for ent in raw_entities:\n",
    "        label = ent.get(\"entity_group\", \"\")\n",
    "        word = ent.get(\"word\", \"\").strip()\n",
    "\n",
    "        # Skip empty/partial tokens\n",
    "        if not word or word.startswith(\"##\"):\n",
    "            continue\n",
    "\n",
    "        for key, valid_labels in label_map.items():\n",
    "            if label.upper() in valid_labels and word not in result[key]:\n",
    "                result[key].append(word)\n",
    "\n",
    "    for key in result:\n",
    "        if return_first_only:\n",
    "            result[key] = result[key][0] if result[key] else None\n",
    "        else:\n",
    "            result[key] = result[key] if result[key] else []\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Regex-Based Extraction (Email, Phone, Skills)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Extract Skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def extract_skills(\n",
    "    text: str,\n",
    "    skill_set: dict = None,\n",
    "    return_freq: bool = False,\n",
    "    return_grouped: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Extract skills from resume or job description text with optional frequency and category grouping.\n",
    "\n",
    "    Args:\n",
    "        text (str): Resume or job description text.\n",
    "        skill_set (dict): Optional skill categories and keywords (dict of lists).\n",
    "        return_freq (bool): If True, return a frequency count of skills.\n",
    "        return_grouped (bool): If True, return categorized skill matches.\n",
    "\n",
    "    Returns:\n",
    "        list or dict: Matched skills as a list, or grouped/frequency dictionary.\n",
    "    \"\"\"\n",
    "\n",
    "    # === 1. Default skill categories and aliases ===\n",
    "    default_skills = {\n",
    "    \"Programming Languages\": [\n",
    "        \"Python\", \"Java\", \"JavaScript\", \"JS\", \"TypeScript\", \"C++\", \"C#\", \"SQL\", \"R\", \"Go\", \"Rust\",\n",
    "        \"Scala\", \"Perl\", \"Ruby\", \"Dart\", \"Objective-C\", \"Swift\", \"Kotlin\"\n",
    "    ],\n",
    "\n",
    "    \"AI / ML / NLP\": [\n",
    "        \"TensorFlow\", \"PyTorch\", \"Keras\", \"Scikit-learn\", \"XGBoost\", \"LightGBM\", \"CatBoost\",\n",
    "        \"Machine Learning\", \"ML\", \"Deep Learning\", \"DL\", \"NLP\", \"LLM\",\n",
    "        \"Transformers\", \"Hugging Face\", \"LangChain\", \"OpenAI API\", \"RAG\", \"Prompt Engineering\",\n",
    "        \"LLM Fine-tuning\", \"AutoML\", \"spaCy\", \"NLTK\", \"BERT\", \"GPT\", \"Llama\", \"Claude\",\n",
    "        \"FAISS\", \"ChromaDB\", \"Weaviate\", \"Haystack\", \"H2O.ai\", \"Vertex AI\"\n",
    "    ],\n",
    "\n",
    "    \"Web Development\": [\n",
    "        \"HTML\", \"CSS\", \"SASS\", \"LESS\", \"Tailwind CSS\", \"Bootstrap\", \"Material-UI\", \"Chakra UI\",\n",
    "        \"React\", \"Next.js\", \"Vue.js\", \"Angular\", \"Node.js\", \"Express\", \"Django\", \"Flask\",\n",
    "        \"FastAPI\", \"ASP.NET\", \"Laravel\", \"Ruby on Rails\", \"REST API\", \"GraphQL\", \"WebSockets\",\n",
    "        \"tRPC\", \"Zustand\", \"Redux\", \"React Query\", \"Vite\", \"Webpack\", \"Parcel\", \"Babel\"\n",
    "    ],\n",
    "\n",
    "    \"Cloud & DevOps\": [\n",
    "        \"AWS\", \"Azure\", \"GCP\", \"DigitalOcean\", \"Heroku\", \"Vercel\", \"Netlify\",\n",
    "        \"Docker\", \"Kubernetes\", \"Helm\", \"Terraform\", \"Ansible\", \"Pulumi\", \"CloudFormation\",\n",
    "        \"GitHub Actions\", \"GitLab CI/CD\", \"Jenkins\", \"CircleCI\", \"ArgoCD\",\n",
    "        \"Linux\", \"Bash\", \"Shell Scripting\", \"Serverless\", \"Prometheus\", \"Grafana\", \"Istio\"\n",
    "    ],\n",
    "\n",
    "    \"Databases\": [\n",
    "        \"MySQL\", \"PostgreSQL\", \"MongoDB\", \"SQLite\", \"BigQuery\", \"Snowflake\", \"Oracle\", \"SQL Server\",\n",
    "        \"Redis\", \"Firestore\", \"Cassandra\", \"DynamoDB\", \"InfluxDB\", \"MariaDB\", \"Redshift\",\n",
    "        \"Neo4j\", \"Supabase\", \"ElasticSearch\", \"DuckDB\"\n",
    "    ],\n",
    "\n",
    "    \"Visualization & BI\": [\n",
    "        \"Power BI\", \"Tableau\", \"Looker\", \"Google Data Studio\", \"Plotly\", \"Dash\",\n",
    "        \"Matplotlib\", \"Seaborn\", \"D3.js\", \"Grafana\", \"Apache Superset\", \"Metabase\"\n",
    "    ],\n",
    "\n",
    "    \"Soft Skills\": [\n",
    "        \"Communication\", \"Problem Solving\", \"Leadership\", \"Teamwork\", \"Adaptability\",\n",
    "        \"Strategic Thinking\", \"Attention to Detail\", \"Time Management\", \"Creativity\",\n",
    "        \"Critical Thinking\", \"Collaboration\", \"Decision Making\", \"Self-Motivation\",\n",
    "        \"Work Ethic\", \"Conflict Resolution\", \"Public Speaking\", \"Presentation Skills\",\n",
    "        \"Mentoring\", \"Accountability\", \"Customer Focus\", \"Project Management\",\n",
    "        \"Empathy\", \"Emotional Intelligence\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "    # === 2. Normalize input ===\n",
    "    text = text.lower()\n",
    "    skills_found = defaultdict(list if return_grouped else int)\n",
    "\n",
    "    # === 3. Match skills ===\n",
    "    skill_categories = skill_set or default_skills\n",
    "\n",
    "    for category, skills in skill_categories.items():\n",
    "        for skill in skills:\n",
    "            skill_lower = skill.lower()\n",
    "            pattern = r'\\b' + re.escape(skill_lower) + r'\\b'\n",
    "            matches = re.findall(pattern, text)\n",
    "            if matches:\n",
    "                if return_grouped:\n",
    "                    skills_found[category].append(skill)\n",
    "                elif return_freq:\n",
    "                    skills_found[skill] += len(matches)\n",
    "                else:\n",
    "                    skills_found[skill] = 1  # just to collect as set\n",
    "\n",
    "    # === 4. Return appropriate output ===\n",
    "    if return_grouped:\n",
    "        return {k: sorted(set(v)) for k, v in skills_found.items() if v}\n",
    "    elif return_freq:\n",
    "        return dict(sorted(skills_found.items(), key=lambda x: -x[1]))\n",
    "    else:\n",
    "        return sorted(skills_found.keys()) if skills_found else None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Extract Education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_education(text, known_degrees=None):\n",
    "    \"\"\"\n",
    "    Extract degrees, institutions, and graduation years from resume text.\n",
    "\n",
    "    Args:\n",
    "        text (str): Resume text content.\n",
    "        known_degrees (list, optional): Custom list of degrees.\n",
    "\n",
    "    Returns:\n",
    "        list: List of dicts with degree, institution, year, and raw line.\n",
    "    \"\"\"\n",
    "    default_degrees = [\n",
    "        \"Bachelor\", \"Master\", \"B.Sc\", \"M.Sc\", \"B.S.\", \"M.S.\", \"BA\", \"MA\", \"PhD\", \"Ph.D\", \"B.E\", \"M.E\",\n",
    "        \"B.Tech\", \"M.Tech\", \"MBA\", \"MCA\", \"BBA\", \"LLB\", \"LLM\", \"MD\", \"DDS\", \"Diploma\", \"High School\",\n",
    "        \"Associate Degree\", \"Doctorate\", \"Postgraduate\", \"Undergraduate\", \"MBBS\", \"CFA\", \"CA\", \"M.Ed\", \"EdD\"\n",
    "    ]\n",
    "\n",
    "    degrees = known_degrees or default_degrees\n",
    "    lines = text.split('\\n')\n",
    "    education_data = []\n",
    "\n",
    "    for line in lines:\n",
    "        line_clean = line.strip()\n",
    "        for degree in degrees:\n",
    "            pattern = rf'\\b{re.escape(degree)}\\b'\n",
    "            if re.search(pattern, line_clean, re.IGNORECASE):\n",
    "                # Try to extract year (4-digit)\n",
    "                year_match = re.search(r'\\b(19|20)\\d{2}\\b', line_clean)\n",
    "                year = year_match.group(0) if year_match else None\n",
    "\n",
    "                # Try to extract institution name heuristically (everything after degree, if any)\n",
    "                after_degree = re.split(pattern, line_clean, flags=re.IGNORECASE)[-1].strip()\n",
    "                institution = after_degree.split(',')[0] if after_degree else None\n",
    "\n",
    "                education_data.append({\n",
    "                    \"degree\": degree,\n",
    "                    \"institution\": institution if institution and not institution.lower().startswith(\"in\") else None,\n",
    "                    \"year\": year,\n",
    "                    \"raw\": line_clean\n",
    "                })\n",
    "                break  # Avoid duplicate matches for the same line\n",
    "\n",
    "    return education_data if education_data else None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Extract Email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_email(text, return_all=False):\n",
    "    \"\"\"\n",
    "    Extract email addresses from text, handling obfuscations like [at], (dot), etc.\n",
    "\n",
    "    Args:\n",
    "        text (str): Raw input text (e.g., resume content).\n",
    "        return_all (bool): If True, return a list of all found emails. Otherwise, return the first one.\n",
    "\n",
    "    Returns:\n",
    "        str or list or None: Extracted email(s), or None if not found.\n",
    "    \"\"\"\n",
    "    if not text or not isinstance(text, str):\n",
    "        return None\n",
    "\n",
    "    # Normalize obfuscated formats (case insensitive)\n",
    "    obfuscations = [\n",
    "        (r'\\s?\\[at\\]\\s?', '@'),\n",
    "        (r'\\s?\\(at\\)\\s?', '@'),\n",
    "        (r'\\s+at\\s+', '@'),\n",
    "        (r'\\s?\\[dot\\]\\s?', '.'),\n",
    "        (r'\\s?\\(dot\\)\\s?', '.'),\n",
    "        (r'\\s+dot\\s+', '.'),\n",
    "    ]\n",
    "\n",
    "    cleaned_text = text.lower()\n",
    "    for pattern, replacement in obfuscations:\n",
    "        cleaned_text = re.sub(pattern, replacement, cleaned_text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Match standard emails (remove trailing punctuation like . or ,)\n",
    "    matches = re.findall(r'[\\w\\.-]+@[\\w\\.-]+\\.\\w+', cleaned_text)\n",
    "    matches = [match.strip(\".,;:\") for match in matches]\n",
    "    matches = sorted(set(matches))\n",
    "\n",
    "    if not matches:\n",
    "        return None\n",
    "\n",
    "    return matches if return_all else matches[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.4 Extract Certifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_certifications(text, known_certs=None, return_lines=False):\n",
    "    \"\"\"\n",
    "    Extract certifications from resume text.\n",
    "\n",
    "    Args:\n",
    "        text (str): Raw resume text.\n",
    "        known_certs (list, optional): List of known cert names/acronyms.\n",
    "        return_lines (bool): If True, return full matched lines; else return cert names only.\n",
    "\n",
    "    Returns:\n",
    "        list: Sorted, deduplicated list of certifications.\n",
    "    \"\"\"\n",
    "    lines = [line.strip(\"•-–—•* \") for line in text.split('\\n') if line.strip()]\n",
    "    results = set()\n",
    "\n",
    "    known_certs = known_certs or [\n",
    "        \"AWS Certified\", \"Google Cloud Certified\", \"Microsoft Certified\", \"Azure Fundamentals\", \"AZ-900\",\n",
    "        \"Certified Scrum Master\", \"Scrum Master\", \"CompTIA A+\", \"CompTIA Security+\", \"CompTIA Network+\",\n",
    "        \"Cisco Certified\", \"CCNA\", \"CKA\", \"CKAD\", \"Oracle Certified\", \"TOGAF\", \"ITIL\", \"CISSP\",\n",
    "        \"Adobe Certified\", \"PMP\", \"PRINCE2\", \"Coursera\", \"Udemy\", \"edX\", \"DataCamp\", \"Trailhead\",\n",
    "        \"IBM Data Science\", \"TensorFlow Developer\", \"Deep Learning Specialization\", \"Salesforce Certified\",\n",
    "        \"LinkedIn Skill Assessment\", \"Superbadge\", \"Kubernetes Mastery\", \"AI For Everyone\", \"OCI Architect\",\n",
    "        \"Google Cloud Professional\", \"Microsoft Azure Fundamentals\", \"OCI 2023 Architect Associate\",\n",
    "        \"Certified Kubernetes Administrator\"\n",
    "    ]\n",
    "\n",
    "    # Fallback pattern to catch any cert/badge/training line\n",
    "    fallback_cert_keywords = re.compile(\n",
    "        r\"(cert(ification|ified)|cert\\.|badge|exam|track|specialization|credential|training|academy)\",\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    for line in lines:\n",
    "        normalized = re.sub(r'[^\\w\\s\\-#@.:/()]', '', line).strip()\n",
    "\n",
    "        # 1. Match known certifications\n",
    "        matched = False\n",
    "        for cert in known_certs:\n",
    "            if cert.lower() in normalized.lower():\n",
    "                results.add(line if return_lines else cert)\n",
    "                matched = True\n",
    "                break\n",
    "\n",
    "        # 2. Match common acronyms\n",
    "        if not matched:\n",
    "            acronyms = re.findall(r'\\b(AZ-\\d{3}|CKA|CKAD|CSM|PMP|CCNA|OCI|CKS|ITIL)\\b', normalized)\n",
    "            for acr in acronyms:\n",
    "                results.add(line if return_lines else acr)\n",
    "\n",
    "        # 3. Match fallback keywords for custom/obscure certifications\n",
    "        if not matched and fallback_cert_keywords.search(normalized):\n",
    "            results.add(line)\n",
    "\n",
    "    return sorted(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 Extract Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_links(text, classify=False, custom_domains=None, strict_mode=False):\n",
    "    \"\"\"\n",
    "    Extract and optionally classify URLs from resume text.\n",
    "\n",
    "    Args:\n",
    "        text (str): Raw resume text.\n",
    "        classify (bool): If True, return a dictionary of categorized links.\n",
    "        custom_domains (dict): Custom classification domains. E.g., {\"Kaggle\": [\"kaggle.com\"]}\n",
    "        strict_mode (bool): If True, ignore partial/bare domains like 'linkedin.com'.\n",
    "\n",
    "    Returns:\n",
    "        list or dict: Cleaned and optionally classified URLs.\n",
    "    \"\"\"\n",
    "    raw_links = set()\n",
    "\n",
    "    # Normalize common obfuscations\n",
    "    text = text.replace(\"[dot]\", \".\").replace(\"(dot)\", \".\").replace(\" dot \", \".\")\n",
    "    text = text.replace(\"[at]\", \"@\").replace(\"(at)\", \"@\").replace(\" at \", \"@\")\n",
    "\n",
    "    # --- 1. Match full links: http(s) ---\n",
    "    full_links = re.findall(r'https?://[^\\s\\)\\]\\>\\.,;]+', text)\n",
    "    raw_links.update(link.strip(\".,);>]\") for link in full_links)\n",
    "\n",
    "    # --- 2. Match www-prefixed domains ---\n",
    "    www_links = re.findall(r'www\\.[\\w\\-\\.]+\\.\\w+', text)\n",
    "    raw_links.update(f\"https://{link.strip('.,);>]')}\" for link in www_links)\n",
    "\n",
    "    # --- 3. Match bare domains (like linkedin.com, github.com) ---\n",
    "    if not strict_mode:\n",
    "        bare_domains = re.findall(r'\\b(?:[\\w\\-]+\\.)+(?:com|org|io|net|co|ai|dev|info)\\b', text, re.IGNORECASE)\n",
    "        raw_links.update(f\"https://{domain.strip('.,);>]')}\" for domain in bare_domains)\n",
    "\n",
    "    clean_links = sorted(set(raw_links))\n",
    "\n",
    "    if not classify:\n",
    "        return clean_links\n",
    "\n",
    "    # --- 4. Classification ---\n",
    "    categories = {\n",
    "        \"LinkedIn\": [],\n",
    "        \"GitHub\": [],\n",
    "        \"Portfolio\": [],\n",
    "        \"Other\": []\n",
    "    }\n",
    "\n",
    "    # Merge custom categories\n",
    "    if custom_domains:\n",
    "        for cat in custom_domains:\n",
    "            categories.setdefault(cat, [])\n",
    "\n",
    "    for link in clean_links:\n",
    "        l = link.lower()\n",
    "        if \"linkedin.com\" in l:\n",
    "            categories[\"LinkedIn\"].append(link)\n",
    "        elif \"github.com\" in l:\n",
    "            categories[\"GitHub\"].append(link)\n",
    "        elif any(sub in l for sub in [\"about.me\", \"portfolio\", \"my.site\", \"personal\", \"me.\"]):\n",
    "            categories[\"Portfolio\"].append(link)\n",
    "        elif custom_domains:\n",
    "            matched = False\n",
    "            for label, domains in custom_domains.items():\n",
    "                if any(domain in l for domain in domains):\n",
    "                    categories[label].append(link)\n",
    "                    matched = True\n",
    "                    break\n",
    "            if not matched:\n",
    "                categories[\"Other\"].append(link)\n",
    "        else:\n",
    "            categories[\"Other\"].append(link)\n",
    "\n",
    "    return categories\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 Extract Phone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_phone(text, return_all=False, format_output=False):\n",
    "    \"\"\"\n",
    "    Extract and optionally format phone numbers from raw text using regex.\n",
    "\n",
    "    Args:\n",
    "        text (str): Raw text (e.g., resume content).\n",
    "        return_all (bool): Return all matched numbers or just the first.\n",
    "        format_output (bool): If True, format numbers like (123) 456-7890.\n",
    "\n",
    "    Returns:\n",
    "        str or list or None: Extracted phone number(s) or None if not found.\n",
    "    \"\"\"\n",
    "    # Common obfuscations & digit words\n",
    "    replacements = {\n",
    "        \"[dot]\": \".\", \"[at]\": \"@\", \" at \": \"@\", \"(at)\": \"@\",\n",
    "        \"zero\": \"0\", \"one\": \"1\", \"two\": \"2\", \"three\": \"3\",\n",
    "        \"four\": \"4\", \"five\": \"5\", \"six\": \"6\", \"seven\": \"7\",\n",
    "        \"eight\": \"8\", \"nine\": \"9\"\n",
    "    }\n",
    "\n",
    "    cleaned = text.lower()\n",
    "    for word, digit in replacements.items():\n",
    "        cleaned = cleaned.replace(word, digit)\n",
    "\n",
    "    # Regex: detect phone numbers (10+ digits, with optional symbols)\n",
    "    potential_numbers = re.findall(r'\\+?\\d[\\d\\s().-]{8,}\\d', cleaned)\n",
    "\n",
    "    results = set()\n",
    "    for raw in potential_numbers:\n",
    "        # Remove unwanted symbols but keep starting '+'\n",
    "        cleaned_number = re.sub(r'(?!^\\+)[^\\d]', '', raw)\n",
    "        if len(cleaned_number) >= 10:\n",
    "            if format_output and cleaned_number.startswith('+'):\n",
    "                formatted = cleaned_number  # leave international numbers as-is\n",
    "            elif format_output:\n",
    "                formatted = f\"({cleaned_number[:3]}) {cleaned_number[3:6]}-{cleaned_number[6:10]}\"\n",
    "            else:\n",
    "                formatted = cleaned_number\n",
    "            results.add(formatted)\n",
    "\n",
    "    sorted_phones = sorted(results)\n",
    "\n",
    "    if not sorted_phones:\n",
    "        return None\n",
    "\n",
    "    return sorted_phones if return_all else sorted_phones[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6 Extract Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links(text, classify=False, custom_domains=None):\n",
    "    \"\"\"\n",
    "    Extract and optionally classify URLs from resume text.\n",
    "\n",
    "    Args:\n",
    "        text (str): Raw resume text.\n",
    "        classify (bool): If True, return a dictionary of categorized links.\n",
    "        custom_domains (dict): Optional dictionary to classify domains. E.g., {\"Kaggle\": [\"kaggle.com\"]}\n",
    "\n",
    "    Returns:\n",
    "        list or dict: Cleaned and optionally classified URLs.\n",
    "    \"\"\"\n",
    "    raw_links = set()\n",
    "\n",
    "    # --- 1. Match standard HTTP/HTTPS links ---\n",
    "    matches_http = re.findall(r'https?://[^\\s\\)\\]\\>\\.,;]*', text)\n",
    "    raw_links.update(link.rstrip(\".,);>]\") for link in matches_http)\n",
    "\n",
    "    # --- 2. Match www. and naked domain names ---\n",
    "    matches_www = re.findall(r'www\\.[a-zA-Z0-9\\-\\.]+\\.\\w+', text)\n",
    "    raw_links.update(f\"https://{link.rstrip('.,);>]')}\" for link in matches_www)\n",
    "\n",
    "    # --- 3. Match obfuscated links like name[dot]com or github[dot]io ---\n",
    "    matches_obfuscated = re.findall(r'[\\w\\-]+\\s?\\[dot\\]\\s?[\\w\\-]+(?:\\s?\\[dot\\]\\s?[\\w\\-]+)?', text, re.IGNORECASE)\n",
    "    for match in matches_obfuscated:\n",
    "        clean = match.replace(\"[dot]\", \".\").replace(\" \", \"\")\n",
    "        if \".\" in clean:\n",
    "            raw_links.add(\"https://\" + clean)\n",
    "\n",
    "    clean_links = sorted(raw_links)\n",
    "\n",
    "    if not classify:\n",
    "        return clean_links\n",
    "\n",
    "    # --- 4. Classification ---\n",
    "    categories = {\n",
    "        \"LinkedIn\": [],\n",
    "        \"GitHub\": [],\n",
    "        \"Portfolio\": [],\n",
    "        \"Other\": []\n",
    "    }\n",
    "\n",
    "    # Merge custom classification if provided\n",
    "    if custom_domains:\n",
    "        for category in custom_domains:\n",
    "            if category not in categories:\n",
    "                categories[category] = []\n",
    "\n",
    "    # Standard + custom match logic\n",
    "    for link in clean_links:\n",
    "        lower = link.lower()\n",
    "        if \"linkedin.com\" in lower:\n",
    "            categories[\"LinkedIn\"].append(link)\n",
    "        elif \"github.com\" in lower:\n",
    "            categories[\"GitHub\"].append(link)\n",
    "        elif any(d in lower for d in [\"about.me\", \"portfolio\", \"me.\", \"my.\", \"personal\"]):\n",
    "            categories[\"Portfolio\"].append(link)\n",
    "        elif custom_domains:\n",
    "            found = False\n",
    "            for label, domains in custom_domains.items():\n",
    "                if any(domain in lower for domain in domains):\n",
    "                    categories[label].append(link)\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                categories[\"Other\"].append(link)\n",
    "        else:\n",
    "            categories[\"Other\"].append(link)\n",
    "\n",
    "    return categories\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.7 Extract Experiance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_experience_lines(text, max_lines=40):\n",
    "    \"\"\"\n",
    "    Extract structured work experience entries from resume text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The full resume text.\n",
    "        max_lines (int): Maximum lines to scan under the 'Experience' section.\n",
    "\n",
    "    Returns:\n",
    "        list: Structured list of experience dictionaries.\n",
    "    \"\"\"\n",
    "    lines = [line.strip() for line in text.split('\\n') if line.strip()]\n",
    "    experience_section = []\n",
    "    section_found = False\n",
    "    keywords = [\"experience\", \"work history\", \"employment\", \"professional background\"]\n",
    "\n",
    "    # Find experience section\n",
    "    for i, line in enumerate(lines):\n",
    "        if not section_found and any(k in line.lower() for k in keywords):\n",
    "            experience_section = lines[i+1:i+1+max_lines]\n",
    "            break\n",
    "\n",
    "    if not experience_section:\n",
    "        return []\n",
    "\n",
    "    experiences = []\n",
    "    current = {}\n",
    "    buffer = []\n",
    "\n",
    "    title_company_pattern = re.compile(\n",
    "        r'^(?P<title>[A-Za-z\\s/()\\-]+?)\\s+[-–]\\s+(?P<company>.+)$'\n",
    "    )\n",
    "    date_pattern = re.compile(r'(\\b\\d{4}\\b).{0,5}(\\bPresent\\b|\\b\\d{4}\\b)', re.IGNORECASE)\n",
    "\n",
    "    for line in experience_section:\n",
    "        if \"education\" in line.lower() or \"certification\" in line.lower() or \"project\" in line.lower():\n",
    "            break  # Stop parsing at next major section\n",
    "\n",
    "        # Match title-company line\n",
    "        tc_match = title_company_pattern.match(line)\n",
    "        date_match = date_pattern.search(line)\n",
    "\n",
    "        if tc_match:\n",
    "            # Save previous block\n",
    "            if current:\n",
    "                current[\"Description\"] = \" \".join(buffer).strip() if buffer else None\n",
    "                experiences.append(current)\n",
    "                buffer = []\n",
    "            current = {\n",
    "                \"Title\": tc_match.group(\"title\").strip(),\n",
    "                \"Company\": tc_match.group(\"company\").strip(),\n",
    "                \"Date\": None,\n",
    "                \"Raw\": line\n",
    "            }\n",
    "\n",
    "        elif date_match:\n",
    "            if current:\n",
    "                current[\"Date\"] = date_match.group(0)\n",
    "\n",
    "        elif line.startswith(\"-\"):\n",
    "            buffer.append(line)\n",
    "\n",
    "    # Final flush\n",
    "    if current:\n",
    "        current[\"Description\"] = \" \".join(buffer).strip() if buffer else None\n",
    "        experiences.append(current)\n",
    "\n",
    "    return experiences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_projects(text, known_skills=None, return_structured=True, max_lines=10):\n",
    "    \"\"\"\n",
    "    Extracts project information from resume text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Resume full text.\n",
    "        known_skills (list): Optional list of tech keywords to extract from descriptions.\n",
    "        return_structured (bool): Return structured dicts with title, description, technologies.\n",
    "        max_lines (int): Max lines to include after a project title.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: List of project dicts (Title, Description, Technologies).\n",
    "    \"\"\"\n",
    "    lines = [line.strip() for line in text.splitlines() if line.strip()]\n",
    "    project_start = None\n",
    "    project_keywords = [\"project\", \"projects\", \"personal project\", \"capstone\"]\n",
    "    stop_keywords = [\"education\", \"experience\", \"certifications\", \"languages\", \"interests\"]\n",
    "\n",
    "    # Find where the Projects section starts\n",
    "    for i, line in enumerate(lines):\n",
    "        if any(kw in line.lower() for kw in project_keywords):\n",
    "            project_start = i + 1\n",
    "            break\n",
    "\n",
    "    if project_start is None:\n",
    "        return []\n",
    "\n",
    "    # Collect all lines after project section until another major section\n",
    "    block = []\n",
    "    for line in lines[project_start:]:\n",
    "        if any(kw in line.lower() for kw in stop_keywords):\n",
    "            break\n",
    "        block.append(line)\n",
    "\n",
    "    # Identify project titles and descriptions\n",
    "    projects = []\n",
    "    current = {\"Title\": None, \"Description\": [], \"Technologies\": []}\n",
    "    title_pattern = re.compile(r'^[A-Z].{3,40}$')  # Heuristic for title (capitalized short line)\n",
    "\n",
    "    for line in block:\n",
    "        if title_pattern.match(line) and not line.startswith(\"-\"):\n",
    "            if current[\"Title\"]:\n",
    "                # Finalize current project\n",
    "                if return_structured:\n",
    "                    current[\"Description\"] = \" \".join(current[\"Description\"]).strip()\n",
    "                    current[\"Technologies\"] = extract_skills(current[\"Description\"], known_skills)\n",
    "                    projects.append(current)\n",
    "                current = {\"Title\": None, \"Description\": [], \"Technologies\": []}\n",
    "            current[\"Title\"] = line\n",
    "        else:\n",
    "            current[\"Description\"].append(line)\n",
    "\n",
    "    # Append last project\n",
    "    if current[\"Title\"]:\n",
    "        current[\"Description\"] = \" \".join(current[\"Description\"]).strip()\n",
    "        current[\"Technologies\"] = extract_skills(current[\"Description\"], known_skills)\n",
    "        projects.append(current)\n",
    "\n",
    "    return projects\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Languages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_languages(text, known_languages=None):\n",
    "    \"\"\"\n",
    "    Extracts spoken or written languages from resume text.\n",
    "\n",
    "    Args:\n",
    "        text (str): Raw resume content.\n",
    "        known_languages (list, optional): Custom list of language names to detect.\n",
    "\n",
    "    Returns:\n",
    "        list: Sorted list of unique language names found in the resume.\n",
    "    \"\"\"\n",
    "    default_languages = [\n",
    "        \"English\", \"Arabic\", \"French\", \"German\", \"Spanish\", \"Italian\", \"Mandarin\",\n",
    "        \"Chinese\", \"Hindi\", \"Japanese\", \"Korean\", \"Portuguese\", \"Russian\", \"Turkish\",\n",
    "        \"Dutch\", \"Bengali\", \"Urdu\", \"Polish\", \"Tamil\", \"Telugu\", \"Swedish\", \"Hebrew\",\n",
    "        \"Malay\", \"Thai\", \"Vietnamese\", \"Greek\", \"Czech\", \"Romanian\", \"Hungarian\",\n",
    "        \"Finnish\", \"Ukrainian\", \"Persian\", \"Punjabi\", \"Serbian\", \"Croatian\"\n",
    "    ]\n",
    "\n",
    "    language_list = known_languages or default_languages\n",
    "    results = set()\n",
    "\n",
    "    # Lower the text for matching\n",
    "    lower_text = text.lower()\n",
    "\n",
    "    for lang in language_list:\n",
    "        pattern = rf\"\\b{re.escape(lang.lower())}\\b\"\n",
    "        if re.search(pattern, lower_text):\n",
    "            results.add(lang)\n",
    "\n",
    "    return sorted(results) if results else None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Hobbies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_interests(text):\n",
    "    \"\"\"\n",
    "    Extract interests or hobbies from resume text.\n",
    "\n",
    "    Args:\n",
    "        text (str): Full resume content.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of extracted interest strings, or None if not found.\n",
    "    \"\"\"\n",
    "    lines = [line.strip() for line in text.split('\\n') if line.strip()]\n",
    "    interests_section = []\n",
    "    start_collecting = False\n",
    "\n",
    "    # Keywords that might indicate start of interests/hobbies section\n",
    "    interest_headers = [\"interests\", \"hobbies\", \"personal interests\", \"activities\"]\n",
    "\n",
    "    for idx, line in enumerate(lines):\n",
    "        lower_line = line.lower().strip()\n",
    "\n",
    "        if any(h in lower_line for h in interest_headers):\n",
    "            start_collecting = True\n",
    "            continue\n",
    "\n",
    "        # Stop collecting if we hit another section (heuristic)\n",
    "        if start_collecting and (re.match(r'^[A-Z][a-zA-Z ]+:$', line) or len(line.split()) <= 2):\n",
    "            break\n",
    "\n",
    "        if start_collecting:\n",
    "            interests_section.append(line)\n",
    "\n",
    "    # Flatten list and remove dashes\n",
    "    cleaned = []\n",
    "    for line in interests_section:\n",
    "        # Remove leading bullet points or dashes\n",
    "        line = re.sub(r'^[-•\\s]+', '', line)\n",
    "        # Split on bullet-like delimiters if needed\n",
    "        parts = re.split(r'[•\\-–•]', line)\n",
    "        for part in parts:\n",
    "            part = part.strip()\n",
    "            if part:\n",
    "                cleaned.append(part)\n",
    "\n",
    "    return cleaned if cleaned else None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Upload File and Extract Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def upload_resume_file():\n",
    "    \"\"\"\n",
    "    Display a file upload widget and return the saved file path from local upload.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing 'file_name' key with the uploaded file path.\n",
    "    \"\"\"\n",
    "    uploaded_file = {\"file_name\": None}  # Shared dict to hold result\n",
    "\n",
    "    uploader = widgets.FileUpload(\n",
    "        accept=\".pdf,.docx,.jpg,.jpeg,.png\",  # Allowed formats\n",
    "        multiple=False\n",
    "    )\n",
    "\n",
    "    display(uploader)\n",
    "\n",
    "    def handle_upload(change):\n",
    "        if uploader.value:\n",
    "            # Access the first uploaded file (assuming one file is uploaded)\n",
    "            uploaded = list(uploader.value.values())[0]\n",
    "            file_name = uploaded['metadata']['name']\n",
    "\n",
    "            # Save the file content to the local disk\n",
    "            with open(file_name, \"wb\") as f:\n",
    "                f.write(uploaded['content'])\n",
    "            print(f\"File saved locally as: {file_name}\")\n",
    "            \n",
    "            # Update the uploaded file dictionary with the file path\n",
    "            uploaded_file[\"file_name\"] = file_name  \n",
    "            uploader.close()\n",
    "\n",
    "    # Observe the file upload event\n",
    "    uploader.observe(handle_upload, names='value')\n",
    "\n",
    "    # Wait until the file is uploaded before returning the result\n",
    "    while uploaded_file[\"file_name\"] is None:\n",
    "        pass  # Keep the loop running until the file is uploaded\n",
    "\n",
    "    return uploaded_file  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Extracted Text in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_extracted_resume_data(parsed_data: dict, output_path: str = \"extracted_resume_data.csv\") -> None:\n",
    "    \"\"\"\n",
    "    Save extracted resume data to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        parsed_data (dict): The dictionary containing resume fields.\n",
    "        output_path (str): Path to save the CSV file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.DataFrame([parsed_data])\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"Resume data saved to: {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save resume data: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Run Extraction Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_resume_data(text):\n",
    "    ner = extract_transformer_entities(text)\n",
    "    return {\n",
    "        **ner,\n",
    "        \"Email\": extract_email(text),\n",
    "        \"Phone\": extract_phone(text),\n",
    "        \"Skills\": extract_skills(text),\n",
    "        \"Education\": extract_education(text),\n",
    "        \"Experience\": extract_experience_lines(text),\n",
    "        \"Certifications\": extract_certifications(text),\n",
    "        \"Projects\": extract_projects(text),\n",
    "        \"Languages\": extract_languages(text),\n",
    "        \"Interests\": extract_interests(text),\n",
    "        \"Links\": extract_links(text),\n",
    "        \"Raw Text\": text,\n",
    "        \"Uploaded At\": datetime.now()\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Resume pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def upload_extract_and_save_resume(save_dir=\"uploads\", output_csv=\"extracted_resume_data.csv\", wait=True):\n",
    "    \"\"\"\n",
    "    Full pipeline to upload, extract, and save resume data.\n",
    "    \n",
    "    1. Uploads a resume file (.pdf or .docx)\n",
    "    2. Extracts fields like name, email, skills, etc.\n",
    "    3. Saves the results to CSV\n",
    "    4. Returns extracted data as dict\n",
    "    \"\"\"\n",
    "    uploaded_file = {\"file_name\": None}\n",
    "\n",
    "    # File upload widget (PDF & DOCX only)\n",
    "    uploader = widgets.FileUpload(\n",
    "        accept=\".pdf,.docx\",\n",
    "        multiple=False\n",
    "    )\n",
    "\n",
    "    display(HTML(\"<h4>Upload your resume (.pdf or .docx):</h4>\"))\n",
    "    display(uploader)\n",
    "\n",
    "    def handle_upload(change):\n",
    "        try:\n",
    "            if uploader.value:\n",
    "                uploaded = list(uploader.value.values())[0]\n",
    "                file_name = uploaded['metadata']['name']\n",
    "                ext = os.path.splitext(file_name)[1].lower()\n",
    "\n",
    "                # Validate file type\n",
    "                if ext not in [\".pdf\", \".docx\"]:\n",
    "                    display(HTML(f\"<span style='color:red;'>❌ Unsupported file type: {ext}</span>\"))\n",
    "                    return\n",
    "\n",
    "                os.makedirs(save_dir, exist_ok=True)\n",
    "                full_path = os.path.join(save_dir, file_name)\n",
    "\n",
    "                with open(full_path, \"wb\") as f:\n",
    "                    f.write(uploaded['content'])\n",
    "\n",
    "                uploaded_file[\"file_name\"] = full_path\n",
    "                display(HTML(f\"<span style='color:green;'>File saved to: {full_path}</span>\"))\n",
    "                uploader.close()\n",
    "\n",
    "                # Step 2: Extract text and parse data\n",
    "                text = extract_resume_text(full_path)\n",
    "\n",
    "                parsed_data = extract_resume_data(text)\n",
    "\n",
    "                # Step 3: Save to CSV\n",
    "                df = pd.DataFrame([parsed_data])\n",
    "                csv_path = os.path.join(save_dir, output_csv)\n",
    "                df.to_csv(csv_path, index=False)\n",
    "\n",
    "                display(HTML(f\"<span style='color:green;'>Data saved to: {csv_path}</span>\"))\n",
    "                display(HTML(\"<h5>Resume Parsing Complete!</h5>\"))\n",
    "                print(parsed_data)\n",
    "\n",
    "            else:\n",
    "                display(HTML(\"<span style='color:red;'>No file uploaded yet.</span>\"))\n",
    "\n",
    "        except Exception as e:\n",
    "            display(HTML(f\"<span style='color:red;'>Error: {e}</span>\"))\n",
    "\n",
    "    uploader.observe(handle_upload, names='value')\n",
    "\n",
    "    if wait:\n",
    "        while uploaded_file[\"file_name\"] is None:\n",
    "            time.sleep(0.2)\n",
    "\n",
    "    return uploaded_file\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53cdf501b92b4b4aa19a50235eed032b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), accept='.pdf,.docx,.jpg,.jpeg,.png', description='Upload')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m resume \u001b[38;5;241m=\u001b[39m \u001b[43mupload_resume_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[20], line 36\u001b[0m, in \u001b[0;36mupload_resume_file\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m uploader\u001b[38;5;241m.\u001b[39mobserve(handle_upload, names\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Wait until the file is uploaded before returning the result\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[43muploaded_file\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfile_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Keep the loop running until the file is uploaded\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m uploaded_file\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "resume = upload_resume_file()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "SUMMRY\n",
       "MAHMoud Al-BANNA\n",
       "Computer Science Teaching Assistant and NLP Researcher\n",
       "-\n",
       "Cairo, Egypt\n",
       "mabdelrazekamin@eelu.edu.eg\n",
       "+201026676511\n",
       "linkedin.com/in/mahmoudalbanna\n",
       "github.com/MahmoudBanna31\n",
       "kaggle.com/mbanna31\n",
       "mahmoudalbanna31@gmail.com\n",
       "\n",
       "\n",
       "Talented Computer Science Teaching Assistant with +4 years of experience in teaching, research, and computer\n",
       "science. A master's student in computer science with a great passion for machine learning, deep learning, data\n",
       "science, and natural language processing and their integration into the medical field.\n",
       "\n",
       "Experience:\n",
       "Egyptian E-Learning University\n",
       "Cairo, Egypt\n",
       "Computer Science Teaching Assistant\n",
       "December 2019 - Present\n",
       "- Faculty of Computers and Information Technology. (Full-Time)\n",
       "- I am teaching these courses. Pattern Recognition Course, Data Structure Course, Introduction to Operation\n",
       "Research Course, Probability and Statistics Course, Mobile and Sensor Networks Course, Web Engineering\n",
       "(3) Course, Introduction to Web Course, Programming (1) Java Basic Course, Programming (3) Java OOP\n",
       "Course, Automata Models Course, Software Engineering (1) Course, Software Engineering (2) Course, Three-\n",
       "dimensional Graphic Course, Computer Graphics Course, Microprocessors and Interfacing Course.\n",
       "- I followed up on these courses. Math (0) Course, Math (1) Course, Physics (1) Course, Human-\n",
       "Computer Interaction Course.\n",
       "- Performed all assistant teaching duties, including mentoring, lecturing, researching, and clerical help.\n",
       "- Organize and facilitate classroom lessons, activities, and presentations for 70+ undergraduate students each\n",
       "semester.\n",
       "- Prepared and delivered lab sessions throughout the academic year.\n",
       "- I am helping the professors to keep records of grades, such as calculating the attendance and term work\n",
       "grades, teaching the practical part in the section, and following up with students till implementation.\n",
       "Arab Open University - Egypt\n",
       "Cairo, Egypt\n",
       "Computer Science Teaching Assistant\n",
       "July 2020 - September 2020\n",
       "- Faculty of Information and Computing. (Part-Time) Computer Science Department.\n",
       "- I taught these Courses: Data Management and Analysis using Python, Database (SQL and NoSQL).\n",
       "- I prepared the lab section of the course to explain it to students and followed up until\n",
       "the implementation.\n",
       "Education:\n",
       "\n",
       "Ain Shams University\n",
       "Cairo, Egypt\n",
       "Faculty of Computer and Information Sciences\n",
       "March 2021 – Present\n",
       "Master's Student in the Computer Science department.\n",
       "Master’s Title “Developing a System for Automatic Text Summarization”.\n",
       "October 2022 - Present\n",
       "Pre-Master’s Courses: -                                                                                                      March 2021 – February 2022\n",
       "Grades: \"Very Good\", GPA: “3.62”\n",
       "- Advanced Software Engineering Course.\n",
       "- Advanced Selected Topics in Computer Science Course.\n",
       "- Advanced Natural Language Processing Course.\n",
       "- Intelligent Computer Algorithm Course.\n",
       "- Advanced Computer Graphics and Animations Course.\n",
       "- Advanced Artificial Intelligence Course.\n",
       "- Distributed Computing Course.\n",
       "- Robotics Course.\n",
       "BSc Information Technology and Computing\n",
       "Open University, UK\n",
       "Grades: Second Class (1st Division) with Honors.\n",
       "Sep 2015 - Jun 2019\n",
       "Arab Open University - Egypt\n",
       "Cairo, Egypt\n",
       "BSc Information Technology and Computers\n",
       "Sep 2015 - Jun 2019\n",
       "Department: Computer Science\n",
       "Grades: \"Very Good with honors\"\n",
       "GPA: “3.65”\n",
       "Class Ranked 4th.\n",
       "SkILLS\n",
       "\n",
       "Programming Languages:\n",
       "Java, Python, JavaScript, C#, Shell Script, C\n",
       "Database:\n",
       "MySQL, XAMPP Server, MongoDB, No-SQL\n",
       "Technologies:\n",
       "Machine Learning, SKLearn, NLTK, TFIDF, Data Preprocessing\n",
       "Concepts:\n",
       "Data Science and Data Engineering, Data Mining, ER Diagram, OOP\n",
       "Principles Concepts:\n",
       "Deep Learning, Data structure and Algorithms, Data Management, and Analysis\n",
       "Concepts:\n",
       "Precision, Recall and F1-Score, Data Warehousing, SOLID Techniques\n",
       "Interpersonal Skills:\n",
       "Communication skills, Time management\n",
       "Projects:\n",
       "\n",
       "Sentiment Analysis for Customer Opinion in the Arabic Language Python, SKLearn, NLTK,\n",
       "TDF-IDF, Machine Learning, Pandas, NumPy\n",
       "I used NLTK and SKLearn for Data preprocessing. Then, using the TF-IDF algorithm for feature extraction.\n",
       "And using different types of classification like SGDClassifier.\n",
       "Software analysis of the hospital in a software engineering course UML Diagram, Java\n",
       "I had an analysis system and determined the functional and non-functional requirements. And, Design\n",
       "solutions such as use case diagrams, class diagrams, and so on.\n",
       "Java OOP Project Java\n",
       "During study course java OOP. I worked on a small project like a calculator.\n",
       "Php Project. Php, MySQL\n",
       "I worked on a small website consisting of some pages like the login page.\n",
       "LICENSES & Certifications:\n",
       "Data Science & Analytics Intro\n",
       "IBM Digital Nation Website\n",
       "Through self-paced learning, this badge earner has displayed an understanding of topics such as Data science,\n",
       "analytics, gathering data, and predicting trends.\n",
       "\n",
       "April 2020\n",
       "Database Fundamentals\n",
       "Mahara Tech-ITI\n",
       "Introduction to the fundamentals of database using MySQL.\n",
       "September 2020\n",
       "Machine Learning Foundations: A Case Study Approach\n",
       "Coursera\n",
       "A case study approach by the University of Washington on Coursera in this course, I get hands-on experience\n",
       "with machine learning from a series of practical case studies.\n",
       "June 2019\n",
       "Interests:\n",
       "Watching Football, Travelling, and Reading.\n",
       "Volunteer: WORKS\n",
       "ACM AOU Community\n",
       "Arab Open University, Egypt\n",
       "Founder of the community. Some of my friends and I at the college have established the first ACM community\n",
       "in the university.\n",
       "Feb 2016 - Dec 2017\n",
       "Enactus AOU\n",
       "Arab Open University, Egypt\n",
       "Member of HR Committee.\n",
       "October 2017 - September 2018\n",
       "IEEE AOU SE\n",
       "Arab Open University, Egypt\n",
       "Human Resources Recruiter.\n",
       "Jun 2017 - March 2018\n",
       "Languages:\n",
       "Arabic: Mother Tongue.\n",
       "English: Professional working proficiency.\n",
       "References:\n",
       "Dr. Walaa Elhady\n",
       "Assistant. Prof. of Information Technology\n",
       "Faculty of Information Technology and Computers, the Egyptian E-Learning University Cairo, Egypt\n",
       "Mobile: +2-01097941593\n",
       "Email: Welhady@eelu.edu.eg\n",
       "Dr. Mustafa Abdul Salam\n",
       "Associate. Prof. of Artificial Intelligence, Former Dean at Arab Open University\n",
       "Faculty of Computers and Artificial Intelligence, Banha University, Arab Open University Cairo, Egypt\n",
       "Mobile: +2-01015372448\n",
       "Email: Mustafa.abdo@ymail.com\n",
       "Dr. Sanaa Taha\n",
       "Associate. Prof. of Information Technology\n",
       "Faculty of Computers and Artificial Intelligence, Cairo University, Cairo, Egypt.\n",
       "Mobile: +2-01117512722, Email: Staha@fci-cu.edu.eg"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resume_path = '../data/resumes/pdf/resume_1.pdf'\n",
    "resume = extract_resume_text(resume_path)\n",
    "display(Markdown(resume))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Regex Based Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Extract Skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AWS',\n",
       " 'Azure',\n",
       " 'BERT',\n",
       " 'CSS',\n",
       " 'Communication',\n",
       " 'Critical Thinking',\n",
       " 'Deep Learning',\n",
       " 'Django',\n",
       " 'Docker',\n",
       " 'FastAPI',\n",
       " 'Flask',\n",
       " 'GCP',\n",
       " 'GitHub Actions',\n",
       " 'Hugging Face',\n",
       " 'JS',\n",
       " 'JavaScript',\n",
       " 'Kubernetes',\n",
       " 'LangChain',\n",
       " 'Leadership',\n",
       " 'ML',\n",
       " 'Machine Learning',\n",
       " 'MongoDB',\n",
       " 'MySQL',\n",
       " 'NLP',\n",
       " 'Next.js',\n",
       " 'Plotly',\n",
       " 'PostgreSQL',\n",
       " 'Problem Solving',\n",
       " 'PyTorch',\n",
       " 'Python',\n",
       " 'REST API',\n",
       " 'React',\n",
       " 'Redis',\n",
       " 'SQL',\n",
       " 'Scikit-learn',\n",
       " 'Tailwind CSS',\n",
       " 'Teamwork',\n",
       " 'Terraform',\n",
       " 'Transformers',\n",
       " 'XGBoost']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Ahmed Mostafa\n",
    "Senior Software Engineer | Backend & ML Specialist\n",
    "Cairo, Egypt | ahmed.dev[at]gmail[dot]com | +20 100 123 4567 | linkedin.com/in/ahmedmostafa | github.com/ahmeddev\n",
    "\n",
    "Summary:\n",
    "Results-driven backend engineer with 6+ years of experience designing scalable systems using Python, FastAPI, PostgreSQL, and Docker. Proficient in building machine learning pipelines, deploying models to production with MLflow and Streamlit. Strong advocate for clean code, CI/CD, and agile development.\n",
    "\n",
    "Skills:\n",
    "- Programming: Python, JavaScript, SQL, C++\n",
    "- ML/AI: Scikit-learn, PyTorch, Transformers, XGBoost, Hugging Face, LangChain\n",
    "- Web: FastAPI, Flask, Django, React, Next.js, Tailwind CSS\n",
    "- Databases: PostgreSQL, MySQL, MongoDB, Redis\n",
    "- DevOps: Docker, Kubernetes, GitHub Actions, Terraform, AWS, GCP\n",
    "- Tools: Jupyter, VS Code, Git, Postman, Slack, Notion\n",
    "- Soft Skills: Problem Solving, Teamwork, Communication, Leadership, Critical Thinking\n",
    "\n",
    "Experience:\n",
    "Senior Backend Engineer – DataStack AI (Remote)\n",
    "Aug 2021 – Present\n",
    "- Built a scalable FastAPI backend for a recommendation engine with PostgreSQL & Redis.\n",
    "- Containerized applications using Docker and deployed to GCP with Kubernetes.\n",
    "- Developed automated CI/CD pipelines using GitHub Actions.\n",
    "- Collaborated cross-functionally with frontend and ML teams using Agile.\n",
    "\n",
    "Machine Learning Engineer – TechNova Labs\n",
    "Jan 2019 – Jul 2021\n",
    "- Deployed NLP models for resume parsing and classification using Hugging Face Transformers.\n",
    "- Trained and tuned models with Scikit-learn, PyTorch, and MLflow.\n",
    "- Developed an interactive data dashboard with Streamlit and Plotly.\n",
    "\n",
    "Education:\n",
    "Bachelor of Computer Science, Cairo University, 2018\n",
    "GPA: 3.7 / 4.0\n",
    "\n",
    "Certifications:\n",
    "- AWS Certified Solutions Architect (2023–2026)\n",
    "- Google Cloud Professional ML Engineer\n",
    "- Microsoft Azure Fundamentals (AZ-900)\n",
    "- Deep Learning Specialization – Coursera (Andrew Ng)\n",
    "- ITIL v4 Foundation\n",
    "- Certified Kubernetes Administrator (CKA)\n",
    "\n",
    "Projects:\n",
    "AI Resume Analyzer\n",
    "- Built a Streamlit app that extracts and analyzes resume data using NLP and BERT NER.\n",
    "- Features: skill matching, email & phone extraction, certification parser.\n",
    "\n",
    "E-commerce REST API\n",
    "- Designed a secure REST API using Django REST Framework.\n",
    "- Integrated Stripe payments and user authentication with JWT.\n",
    "\n",
    "Languages:\n",
    "- Arabic (Native)\n",
    "- English (Fluent)\n",
    "- German (Intermediate)\n",
    "\n",
    "Interests:\n",
    "- Open source contributions\n",
    "- AI ethics & fairness\n",
    "- Playing chess & learning languages\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "extract_skills(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Extract Education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Education Degrees: [{'degree': 'Bachelor', 'institution': 'of Computer Science', 'year': '2018', 'raw': 'Bachelor of Computer Science, Cairo University, 2018'}]\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Ahmed Mostafa\n",
    "Senior Software Engineer | Backend & ML Specialist\n",
    "Cairo, Egypt | ahmed.dev[at]gmail[dot]com | +20 100 123 4567 | linkedin.com/in/ahmedmostafa | github.com/ahmeddev\n",
    "\n",
    "Summary:\n",
    "Results-driven backend engineer with 6+ years of experience designing scalable systems using Python, FastAPI, PostgreSQL, and Docker. Proficient in building machine learning pipelines, deploying models to production with MLflow and Streamlit. Strong advocate for clean code, CI/CD, and agile development.\n",
    "\n",
    "Skills:\n",
    "- Programming: Python, JavaScript, SQL, C++\n",
    "- ML/AI: Scikit-learn, PyTorch, Transformers, XGBoost, Hugging Face, LangChain\n",
    "- Web: FastAPI, Flask, Django, React, Next.js, Tailwind CSS\n",
    "- Databases: PostgreSQL, MySQL, MongoDB, Redis\n",
    "- DevOps: Docker, Kubernetes, GitHub Actions, Terraform, AWS, GCP\n",
    "- Tools: Jupyter, VS Code, Git, Postman, Slack, Notion\n",
    "- Soft Skills: Problem Solving, Teamwork, Communication, Leadership, Critical Thinking\n",
    "\n",
    "Experience:\n",
    "Senior Backend Engineer – DataStack AI (Remote)\n",
    "Aug 2021 – Present\n",
    "- Built a scalable FastAPI backend for a recommendation engine with PostgreSQL & Redis.\n",
    "- Containerized applications using Docker and deployed to GCP with Kubernetes.\n",
    "- Developed automated CI/CD pipelines using GitHub Actions.\n",
    "- Collaborated cross-functionally with frontend and ML teams using Agile.\n",
    "\n",
    "Machine Learning Engineer – TechNova Labs\n",
    "Jan 2019 – Jul 2021\n",
    "- Deployed NLP models for resume parsing and classification using Hugging Face Transformers.\n",
    "- Trained and tuned models with Scikit-learn, PyTorch, and MLflow.\n",
    "- Developed an interactive data dashboard with Streamlit and Plotly.\n",
    "\n",
    "Education:\n",
    "Bachelor of Computer Science, Cairo University, 2018\n",
    "GPA: 3.7 / 4.0\n",
    "\n",
    "Certifications:\n",
    "- AWS Certified Solutions Architect (2023–2026)\n",
    "- Google Cloud Professional ML Engineer\n",
    "- Microsoft Azure Fundamentals (AZ-900)\n",
    "- Deep Learning Specialization – Coursera (Andrew Ng)\n",
    "- ITIL v4 Foundation\n",
    "- Certified Kubernetes Administrator (CKA)\n",
    "\n",
    "Projects:\n",
    "AI Resume Analyzer\n",
    "- Built a Streamlit app that extracts and analyzes resume data using NLP and BERT NER.\n",
    "- Features: skill matching, email & phone extraction, certification parser.\n",
    "\n",
    "E-commerce REST API\n",
    "- Designed a secure REST API using Django REST Framework.\n",
    "- Integrated Stripe payments and user authentication with JWT.\n",
    "\n",
    "Languages:\n",
    "- Arabic (Native)\n",
    "- English (Fluent)\n",
    "- German (Intermediate)\n",
    "\n",
    "Interests:\n",
    "- Open source contributions\n",
    "- AI ethics & fairness\n",
    "- Playing chess & learning languages\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "education_extracted = extract_education(text)\n",
    "print(\"Extracted Education Degrees:\", education_extracted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Extract Email "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ahmed.dev@gmail.com'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Ahmed Mostafa\n",
    "Senior Software Engineer | Backend & ML Specialist\n",
    "Cairo, Egypt | ahmed.dev[at]gmail[dot]com | +20 100 123 4567 | linkedin.com/in/ahmedmostafa | github.com/ahmeddev\n",
    "\n",
    "Summary:\n",
    "Results-driven backend engineer with 6+ years of experience designing scalable systems using Python, FastAPI, PostgreSQL, and Docker. Proficient in building machine learning pipelines, deploying models to production with MLflow and Streamlit. Strong advocate for clean code, CI/CD, and agile development.\n",
    "\n",
    "Skills:\n",
    "- Programming: Python, JavaScript, SQL, C++\n",
    "- ML/AI: Scikit-learn, PyTorch, Transformers, XGBoost, Hugging Face, LangChain\n",
    "- Web: FastAPI, Flask, Django, React, Next.js, Tailwind CSS\n",
    "- Databases: PostgreSQL, MySQL, MongoDB, Redis\n",
    "- DevOps: Docker, Kubernetes, GitHub Actions, Terraform, AWS, GCP\n",
    "- Tools: Jupyter, VS Code, Git, Postman, Slack, Notion\n",
    "- Soft Skills: Problem Solving, Teamwork, Communication, Leadership, Critical Thinking\n",
    "\n",
    "Experience:\n",
    "Senior Backend Engineer – DataStack AI (Remote)\n",
    "Aug 2021 – Present\n",
    "- Built a scalable FastAPI backend for a recommendation engine with PostgreSQL & Redis.\n",
    "- Containerized applications using Docker and deployed to GCP with Kubernetes.\n",
    "- Developed automated CI/CD pipelines using GitHub Actions.\n",
    "- Collaborated cross-functionally with frontend and ML teams using Agile.\n",
    "\n",
    "Machine Learning Engineer – TechNova Labs\n",
    "Jan 2019 – Jul 2021\n",
    "- Deployed NLP models for resume parsing and classification using Hugging Face Transformers.\n",
    "- Trained and tuned models with Scikit-learn, PyTorch, and MLflow.\n",
    "- Developed an interactive data dashboard with Streamlit and Plotly.\n",
    "\n",
    "Education:\n",
    "Bachelor of Computer Science, Cairo University, 2018\n",
    "GPA: 3.7 / 4.0\n",
    "\n",
    "Certifications:\n",
    "- AWS Certified Solutions Architect (2023–2026)\n",
    "- Google Cloud Professional ML Engineer\n",
    "- Microsoft Azure Fundamentals (AZ-900)\n",
    "- Deep Learning Specialization – Coursera (Andrew Ng)\n",
    "- ITIL v4 Foundation\n",
    "- Certified Kubernetes Administrator (CKA)\n",
    "\n",
    "Projects:\n",
    "AI Resume Analyzer\n",
    "- Built a Streamlit app that extracts and analyzes resume data using NLP and BERT NER.\n",
    "- Features: skill matching, email & phone extraction, certification parser.\n",
    "\n",
    "E-commerce REST API\n",
    "- Designed a secure REST API using Django REST Framework.\n",
    "- Integrated Stripe payments and user authentication with JWT.\n",
    "\n",
    "Languages:\n",
    "- Arabic (Native)\n",
    "- English (Fluent)\n",
    "- German (Intermediate)\n",
    "\n",
    "Interests:\n",
    "- Open source contributions\n",
    "- AI ethics & fairness\n",
    "- Playing chess & learning languages\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "extract_email(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Extract Certifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AWS Certified',\n",
       " 'Azure Fundamentals',\n",
       " 'CKA',\n",
       " 'Certifications:',\n",
       " 'Coursera',\n",
       " 'Features: skill matching, email & phone extraction, certification parser.',\n",
       " 'Google Cloud Professional',\n",
       " 'ITIL']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Ahmed Mostafa\n",
    "Senior Software Engineer | Backend & ML Specialist\n",
    "Cairo, Egypt | ahmed.dev[at]gmail[dot]com | +20 100 123 4567 | linkedin.com/in/ahmedmostafa | github.com/ahmeddev\n",
    "\n",
    "Summary:\n",
    "Results-driven backend engineer with 6+ years of experience designing scalable systems using Python, FastAPI, PostgreSQL, and Docker. Proficient in building machine learning pipelines, deploying models to production with MLflow and Streamlit. Strong advocate for clean code, CI/CD, and agile development.\n",
    "\n",
    "Skills:\n",
    "- Programming: Python, JavaScript, SQL, C++\n",
    "- ML/AI: Scikit-learn, PyTorch, Transformers, XGBoost, Hugging Face, LangChain\n",
    "- Web: FastAPI, Flask, Django, React, Next.js, Tailwind CSS\n",
    "- Databases: PostgreSQL, MySQL, MongoDB, Redis\n",
    "- DevOps: Docker, Kubernetes, GitHub Actions, Terraform, AWS, GCP\n",
    "- Tools: Jupyter, VS Code, Git, Postman, Slack, Notion\n",
    "- Soft Skills: Problem Solving, Teamwork, Communication, Leadership, Critical Thinking\n",
    "\n",
    "Experience:\n",
    "Senior Backend Engineer – DataStack AI (Remote)\n",
    "Aug 2021 – Present\n",
    "- Built a scalable FastAPI backend for a recommendation engine with PostgreSQL & Redis.\n",
    "- Containerized applications using Docker and deployed to GCP with Kubernetes.\n",
    "- Developed automated CI/CD pipelines using GitHub Actions.\n",
    "- Collaborated cross-functionally with frontend and ML teams using Agile.\n",
    "\n",
    "Machine Learning Engineer – TechNova Labs\n",
    "Jan 2019 – Jul 2021\n",
    "- Deployed NLP models for resume parsing and classification using Hugging Face Transformers.\n",
    "- Trained and tuned models with Scikit-learn, PyTorch, and MLflow.\n",
    "- Developed an interactive data dashboard with Streamlit and Plotly.\n",
    "\n",
    "Education:\n",
    "Bachelor of Computer Science, Cairo University, 2018\n",
    "GPA: 3.7 / 4.0\n",
    "\n",
    "Certifications:\n",
    "- AWS Certified Solutions Architect (2023–2026)\n",
    "- Google Cloud Professional ML Engineer\n",
    "- Microsoft Azure Fundamentals (AZ-900)\n",
    "- Deep Learning Specialization – Coursera (Andrew Ng)\n",
    "- ITIL v4 Foundation\n",
    "- Certified Kubernetes Administrator (CKA)\n",
    "\n",
    "Projects:\n",
    "AI Resume Analyzer\n",
    "- Built a Streamlit app that extracts and analyzes resume data using NLP and BERT NER.\n",
    "- Features: skill matching, email & phone extraction, certification parser.\n",
    "\n",
    "E-commerce REST API\n",
    "- Designed a secure REST API using Django REST Framework.\n",
    "- Integrated Stripe payments and user authentication with JWT.\n",
    "\n",
    "Languages:\n",
    "- Arabic (Native)\n",
    "- English (Fluent)\n",
    "- German (Intermediate)\n",
    "\n",
    "Interests:\n",
    "- Open source contributions\n",
    "- AI ethics & fairness\n",
    "- Playing chess & learning languages\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "extract_certifications(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.5 Extract Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://gmail.com']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Ahmed Mostafa\n",
    "Senior Software Engineer | Backend & ML Specialist\n",
    "Cairo, Egypt | ahmed.dev[at]gmail[dot]com | +20 100 123 4567 | linkedin.com/in/ahmedmostafa | github.com/ahmeddev\n",
    "\n",
    "Summary:\n",
    "Results-driven backend engineer with 6+ years of experience designing scalable systems using Python, FastAPI, PostgreSQL, and Docker. Proficient in building machine learning pipelines, deploying models to production with MLflow and Streamlit. Strong advocate for clean code, CI/CD, and agile development.\n",
    "\n",
    "Skills:\n",
    "- Programming: Python, JavaScript, SQL, C++\n",
    "- ML/AI: Scikit-learn, PyTorch, Transformers, XGBoost, Hugging Face, LangChain\n",
    "- Web: FastAPI, Flask, Django, React, Next.js, Tailwind CSS\n",
    "- Databases: PostgreSQL, MySQL, MongoDB, Redis\n",
    "- DevOps: Docker, Kubernetes, GitHub Actions, Terraform, AWS, GCP\n",
    "- Tools: Jupyter, VS Code, Git, Postman, Slack, Notion\n",
    "- Soft Skills: Problem Solving, Teamwork, Communication, Leadership, Critical Thinking\n",
    "\n",
    "Experience:\n",
    "Senior Backend Engineer – DataStack AI (Remote)\n",
    "Aug 2021 – Present\n",
    "- Built a scalable FastAPI backend for a recommendation engine with PostgreSQL & Redis.\n",
    "- Containerized applications using Docker and deployed to GCP with Kubernetes.\n",
    "- Developed automated CI/CD pipelines using GitHub Actions.\n",
    "- Collaborated cross-functionally with frontend and ML teams using Agile.\n",
    "\n",
    "Machine Learning Engineer – TechNova Labs\n",
    "Jan 2019 – Jul 2021\n",
    "- Deployed NLP models for resume parsing and classification using Hugging Face Transformers.\n",
    "- Trained and tuned models with Scikit-learn, PyTorch, and MLflow.\n",
    "- Developed an interactive data dashboard with Streamlit and Plotly.\n",
    "\n",
    "Education:\n",
    "Bachelor of Computer Science, Cairo University, 2018\n",
    "GPA: 3.7 / 4.0\n",
    "\n",
    "Certifications:\n",
    "- AWS Certified Solutions Architect (2023–2026)\n",
    "- Google Cloud Professional ML Engineer\n",
    "- Microsoft Azure Fundamentals (AZ-900)\n",
    "- Deep Learning Specialization – Coursera (Andrew Ng)\n",
    "- ITIL v4 Foundation\n",
    "- Certified Kubernetes Administrator (CKA)\n",
    "\n",
    "Projects:\n",
    "AI Resume Analyzer\n",
    "- Built a Streamlit app that extracts and analyzes resume data using NLP and BERT NER.\n",
    "- Features: skill matching, email & phone extraction, certification parser.\n",
    "\n",
    "E-commerce REST API\n",
    "- Designed a secure REST API using Django REST Framework.\n",
    "- Integrated Stripe payments and user authentication with JWT.\n",
    "\n",
    "Languages:\n",
    "- Arabic (Native)\n",
    "- English (Fluent)\n",
    "- German (Intermediate)\n",
    "\n",
    "Interests:\n",
    "- Open source contributions\n",
    "- AI ethics & fairness\n",
    "- Playing chess & learning languages\n",
    "\"\"\"\n",
    "\n",
    "extract_links(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Extract Phone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'+201001234567'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Ahmed Mostafa\n",
    "Senior Software Engineer | Backend & ML Specialist\n",
    "Cairo, Egypt | ahmed.dev[at]gmail[dot]com | +20 100 123 4567 | linkedin.com/in/ahmedmostafa | github.com/ahmeddev\n",
    "\n",
    "Summary:\n",
    "Results-driven backend engineer with 6+ years of experience designing scalable systems using Python, FastAPI, PostgreSQL, and Docker. Proficient in building machine learning pipelines, deploying models to production with MLflow and Streamlit. Strong advocate for clean code, CI/CD, and agile development.\n",
    "\n",
    "Skills:\n",
    "- Programming: Python, JavaScript, SQL, C++\n",
    "- ML/AI: Scikit-learn, PyTorch, Transformers, XGBoost, Hugging Face, LangChain\n",
    "- Web: FastAPI, Flask, Django, React, Next.js, Tailwind CSS\n",
    "- Databases: PostgreSQL, MySQL, MongoDB, Redis\n",
    "- DevOps: Docker, Kubernetes, GitHub Actions, Terraform, AWS, GCP\n",
    "- Tools: Jupyter, VS Code, Git, Postman, Slack, Notion\n",
    "- Soft Skills: Problem Solving, Teamwork, Communication, Leadership, Critical Thinking\n",
    "\n",
    "Experience:\n",
    "Senior Backend Engineer – DataStack AI (Remote)\n",
    "Aug 2021 – Present\n",
    "- Built a scalable FastAPI backend for a recommendation engine with PostgreSQL & Redis.\n",
    "- Containerized applications using Docker and deployed to GCP with Kubernetes.\n",
    "- Developed automated CI/CD pipelines using GitHub Actions.\n",
    "- Collaborated cross-functionally with frontend and ML teams using Agile.\n",
    "\n",
    "Machine Learning Engineer – TechNova Labs\n",
    "Jan 2019 – Jul 2021\n",
    "- Deployed NLP models for resume parsing and classification using Hugging Face Transformers.\n",
    "- Trained and tuned models with Scikit-learn, PyTorch, and MLflow.\n",
    "- Developed an interactive data dashboard with Streamlit and Plotly.\n",
    "\n",
    "Education:\n",
    "Bachelor of Computer Science, Cairo University, 2018\n",
    "GPA: 3.7 / 4.0\n",
    "\n",
    "Certifications:\n",
    "- AWS Certified Solutions Architect (2023–2026)\n",
    "- Google Cloud Professional ML Engineer\n",
    "- Microsoft Azure Fundamentals (AZ-900)\n",
    "- Deep Learning Specialization – Coursera (Andrew Ng)\n",
    "- ITIL v4 Foundation\n",
    "- Certified Kubernetes Administrator (CKA)\n",
    "\n",
    "Projects:\n",
    "AI Resume Analyzer\n",
    "- Built a Streamlit app that extracts and analyzes resume data using NLP and BERT NER.\n",
    "- Features: skill matching, email & phone extraction, certification parser.\n",
    "\n",
    "E-commerce REST API\n",
    "- Designed a secure REST API using Django REST Framework.\n",
    "- Integrated Stripe payments and user authentication with JWT.\n",
    "\n",
    "Languages:\n",
    "- Arabic (Native)\n",
    "- English (Fluent)\n",
    "- German (Intermediate)\n",
    "\n",
    "Interests:\n",
    "- Open source contributions\n",
    "- AI ethics & fairness\n",
    "- Playing chess & learning languages\n",
    "\"\"\"\n",
    "\n",
    "extract_phone(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Extract Experiance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Title': 'Senior Backend Engineer',\n",
       "  'Company': 'DataStack AI (Remote)',\n",
       "  'Date': '2021 – Present',\n",
       "  'Raw': 'Senior Backend Engineer – DataStack AI (Remote)',\n",
       "  'Description': '- Programming: Python, JavaScript, SQL, C++ - ML/AI: Scikit-learn, PyTorch, Transformers, XGBoost, Hugging Face, LangChain - Web: FastAPI, Flask, Django, React, Next.js, Tailwind CSS - Databases: PostgreSQL, MySQL, MongoDB, Redis - DevOps: Docker, Kubernetes, GitHub Actions, Terraform, AWS, GCP - Tools: Jupyter, VS Code, Git, Postman, Slack, Notion - Soft Skills: Problem Solving, Teamwork, Communication, Leadership, Critical Thinking - Built a scalable FastAPI backend for a recommendation engine with PostgreSQL & Redis. - Containerized applications using Docker and deployed to GCP with Kubernetes. - Developed automated CI/CD pipelines using GitHub Actions. - Collaborated cross-functionally with frontend and ML teams using Agile.'},\n",
       " {'Title': 'Machine Learning Engineer',\n",
       "  'Company': 'TechNova Labs',\n",
       "  'Date': None,\n",
       "  'Raw': 'Machine Learning Engineer – TechNova Labs',\n",
       "  'Description': '- Deployed NLP models for resume parsing and classification using Hugging Face Transformers. - Trained and tuned models with Scikit-learn, PyTorch, and MLflow. - Developed an interactive data dashboard with Streamlit and Plotly.'}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Ahmed Mostafa\n",
    "Senior Software Engineer | Backend & ML Specialist\n",
    "Cairo, Egypt | ahmed.dev[at]gmail[dot]com | +20 100 123 4567 | linkedin.com/in/ahmedmostafa | github.com/ahmeddev\n",
    "\n",
    "Summary:\n",
    "Results-driven backend engineer with 6+ years of experience designing scalable systems using Python, FastAPI, PostgreSQL, and Docker. Proficient in building machine learning pipelines, deploying models to production with MLflow and Streamlit. Strong advocate for clean code, CI/CD, and agile development.\n",
    "\n",
    "Skills:\n",
    "- Programming: Python, JavaScript, SQL, C++\n",
    "- ML/AI: Scikit-learn, PyTorch, Transformers, XGBoost, Hugging Face, LangChain\n",
    "- Web: FastAPI, Flask, Django, React, Next.js, Tailwind CSS\n",
    "- Databases: PostgreSQL, MySQL, MongoDB, Redis\n",
    "- DevOps: Docker, Kubernetes, GitHub Actions, Terraform, AWS, GCP\n",
    "- Tools: Jupyter, VS Code, Git, Postman, Slack, Notion\n",
    "- Soft Skills: Problem Solving, Teamwork, Communication, Leadership, Critical Thinking\n",
    "\n",
    "Experience:\n",
    "Senior Backend Engineer – DataStack AI (Remote)\n",
    "Aug 2021 – Present\n",
    "- Built a scalable FastAPI backend for a recommendation engine with PostgreSQL & Redis.\n",
    "- Containerized applications using Docker and deployed to GCP with Kubernetes.\n",
    "- Developed automated CI/CD pipelines using GitHub Actions.\n",
    "- Collaborated cross-functionally with frontend and ML teams using Agile.\n",
    "\n",
    "Machine Learning Engineer – TechNova Labs\n",
    "Jan 2019 – Jul 2021\n",
    "- Deployed NLP models for resume parsing and classification using Hugging Face Transformers.\n",
    "- Trained and tuned models with Scikit-learn, PyTorch, and MLflow.\n",
    "- Developed an interactive data dashboard with Streamlit and Plotly.\n",
    "\n",
    "Education:\n",
    "Bachelor of Computer Science, Cairo University, 2018\n",
    "GPA: 3.7 / 4.0\n",
    "\n",
    "Certifications:\n",
    "- AWS Certified Solutions Architect (2023–2026)\n",
    "- Google Cloud Professional ML Engineer\n",
    "- Microsoft Azure Fundamentals (AZ-900)\n",
    "- Deep Learning Specialization – Coursera (Andrew Ng)\n",
    "- ITIL v4 Foundation\n",
    "- Certified Kubernetes Administrator (CKA)\n",
    "\n",
    "Projects:\n",
    "AI Resume Analyzer\n",
    "- Built a Streamlit app that extracts and analyzes resume data using NLP and BERT NER.\n",
    "- Features: skill matching, email & phone extraction, certification parser.\n",
    "\n",
    "E-commerce REST API\n",
    "- Designed a secure REST API using Django REST Framework.\n",
    "- Integrated Stripe payments and user authentication with JWT.\n",
    "\n",
    "Languages:\n",
    "- Arabic (Native)\n",
    "- English (Fluent)\n",
    "- German (Intermediate)\n",
    "\n",
    "Interests:\n",
    "- Open source contributions\n",
    "- AI ethics & fairness\n",
    "- Playing chess & learning languages\n",
    "\"\"\"\n",
    "\n",
    "extract_experience_lines(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 Extract Projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Title': 'AI Resume Analyzer',\n",
       "  'Description': '- Built a Streamlit app that extracts and analyzes resume data using NLP and BERT NER. - Features: skill matching, email & phone extraction, certification parser.',\n",
       "  'Technologies': ['BERT', 'NLP']},\n",
       " {'Title': 'E-commerce REST API',\n",
       "  'Description': '- Designed a secure REST API using Django REST Framework. - Integrated Stripe payments and user authentication with JWT.',\n",
       "  'Technologies': ['Django', 'REST API']}]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Ahmed Mostafa\n",
    "Senior Software Engineer | Backend & ML Specialist\n",
    "Cairo, Egypt | ahmed.dev[at]gmail[dot]com | +20 100 123 4567 | linkedin.com/in/ahmedmostafa | github.com/ahmeddev\n",
    "\n",
    "Summary:\n",
    "Results-driven backend engineer with 6+ years of experience designing scalable systems using Python, FastAPI, PostgreSQL, and Docker. Proficient in building machine learning pipelines, deploying models to production with MLflow and Streamlit. Strong advocate for clean code, CI/CD, and agile development.\n",
    "\n",
    "Skills:\n",
    "- Programming: Python, JavaScript, SQL, C++\n",
    "- ML/AI: Scikit-learn, PyTorch, Transformers, XGBoost, Hugging Face, LangChain\n",
    "- Web: FastAPI, Flask, Django, React, Next.js, Tailwind CSS\n",
    "- Databases: PostgreSQL, MySQL, MongoDB, Redis\n",
    "- DevOps: Docker, Kubernetes, GitHub Actions, Terraform, AWS, GCP\n",
    "- Tools: Jupyter, VS Code, Git, Postman, Slack, Notion\n",
    "- Soft Skills: Problem Solving, Teamwork, Communication, Leadership, Critical Thinking\n",
    "\n",
    "Experience:\n",
    "Senior Backend Engineer – DataStack AI (Remote)\n",
    "Aug 2021 – Present\n",
    "- Built a scalable FastAPI backend for a recommendation engine with PostgreSQL & Redis.\n",
    "- Containerized applications using Docker and deployed to GCP with Kubernetes.\n",
    "- Developed automated CI/CD pipelines using GitHub Actions.\n",
    "- Collaborated cross-functionally with frontend and ML teams using Agile.\n",
    "\n",
    "Machine Learning Engineer – TechNova Labs\n",
    "Jan 2019 – Jul 2021\n",
    "- Deployed NLP models for resume parsing and classification using Hugging Face Transformers.\n",
    "- Trained and tuned models with Scikit-learn, PyTorch, and MLflow.\n",
    "- Developed an interactive data dashboard with Streamlit and Plotly.\n",
    "\n",
    "Education:\n",
    "Bachelor of Computer Science, Cairo University, 2018\n",
    "GPA: 3.7 / 4.0\n",
    "\n",
    "Certifications:\n",
    "- AWS Certified Solutions Architect (2023–2026)\n",
    "- Google Cloud Professional ML Engineer\n",
    "- Microsoft Azure Fundamentals (AZ-900)\n",
    "- Deep Learning Specialization – Coursera (Andrew Ng)\n",
    "- ITIL v4 Foundation\n",
    "- Certified Kubernetes Administrator (CKA)\n",
    "\n",
    "Projects:\n",
    "AI Resume Analyzer\n",
    "- Built a Streamlit app that extracts and analyzes resume data using NLP and BERT NER.\n",
    "- Features: skill matching, email & phone extraction, certification parser.\n",
    "\n",
    "E-commerce REST API\n",
    "- Designed a secure REST API using Django REST Framework.\n",
    "- Integrated Stripe payments and user authentication with JWT.\n",
    "\n",
    "Languages:\n",
    "- Arabic (Native)\n",
    "- English (Fluent)\n",
    "- German (Intermediate)\n",
    "\n",
    "Interests:\n",
    "- Open source contributions\n",
    "- AI ethics & fairness\n",
    "- Playing chess & learning languages\n",
    "\"\"\"\n",
    "\n",
    "extract_projects(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.9 Extract Hobbies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Open source contributions',\n",
       " 'AI ethics & fairness',\n",
       " 'Playing chess & learning languages']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Ahmed Mostafa\n",
    "Senior Software Engineer | Backend & ML Specialist\n",
    "Cairo, Egypt | ahmed.dev[at]gmail[dot]com | +20 100 123 4567 | linkedin.com/in/ahmedmostafa | github.com/ahmeddev\n",
    "\n",
    "Summary:\n",
    "Results-driven backend engineer with 6+ years of experience designing scalable systems using Python, FastAPI, PostgreSQL, and Docker. Proficient in building machine learning pipelines, deploying models to production with MLflow and Streamlit. Strong advocate for clean code, CI/CD, and agile development.\n",
    "\n",
    "Skills:\n",
    "- Programming: Python, JavaScript, SQL, C++\n",
    "- ML/AI: Scikit-learn, PyTorch, Transformers, XGBoost, Hugging Face, LangChain\n",
    "- Web: FastAPI, Flask, Django, React, Next.js, Tailwind CSS\n",
    "- Databases: PostgreSQL, MySQL, MongoDB, Redis\n",
    "- DevOps: Docker, Kubernetes, GitHub Actions, Terraform, AWS, GCP\n",
    "- Tools: Jupyter, VS Code, Git, Postman, Slack, Notion\n",
    "- Soft Skills: Problem Solving, Teamwork, Communication, Leadership, Critical Thinking\n",
    "\n",
    "Experience:\n",
    "Senior Backend Engineer – DataStack AI (Remote)\n",
    "Aug 2021 – Present\n",
    "- Built a scalable FastAPI backend for a recommendation engine with PostgreSQL & Redis.\n",
    "- Containerized applications using Docker and deployed to GCP with Kubernetes.\n",
    "- Developed automated CI/CD pipelines using GitHub Actions.\n",
    "- Collaborated cross-functionally with frontend and ML teams using Agile.\n",
    "\n",
    "Machine Learning Engineer – TechNova Labs\n",
    "Jan 2019 – Jul 2021\n",
    "- Deployed NLP models for resume parsing and classification using Hugging Face Transformers.\n",
    "- Trained and tuned models with Scikit-learn, PyTorch, and MLflow.\n",
    "- Developed an interactive data dashboard with Streamlit and Plotly.\n",
    "\n",
    "Education:\n",
    "Bachelor of Computer Science, Cairo University, 2018\n",
    "GPA: 3.7 / 4.0\n",
    "\n",
    "Certifications:\n",
    "- AWS Certified Solutions Architect (2023–2026)\n",
    "- Google Cloud Professional ML Engineer\n",
    "- Microsoft Azure Fundamentals (AZ-900)\n",
    "- Deep Learning Specialization – Coursera (Andrew Ng)\n",
    "- ITIL v4 Foundation\n",
    "- Certified Kubernetes Administrator (CKA)\n",
    "\n",
    "Projects:\n",
    "AI Resume Analyzer\n",
    "- Built a Streamlit app that extracts and analyzes resume data using NLP and BERT NER.\n",
    "- Features: skill matching, email & phone extraction, certification parser.\n",
    "\n",
    "E-commerce REST API\n",
    "- Designed a secure REST API using Django REST Framework.\n",
    "- Integrated Stripe payments and user authentication with JWT.\n",
    "\n",
    "Languages:\n",
    "- Arabic (Native)\n",
    "- English (Fluent)\n",
    "- German (Intermediate)\n",
    "\n",
    "Interests:\n",
    "- Open source contributions\n",
    "- AI ethics & fairness\n",
    "- Playing chess & learning languages\n",
    "\"\"\"\n",
    "\n",
    "extract_interests(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 10 Extract Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Arabic', 'English', 'German']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Ahmed Mostafa\n",
    "Senior Software Engineer | Backend & ML Specialist\n",
    "Cairo, Egypt | ahmed.dev[at]gmail[dot]com | +20 100 123 4567 | linkedin.com/in/ahmedmostafa | github.com/ahmeddev\n",
    "\n",
    "Summary:\n",
    "Results-driven backend engineer with 6+ years of experience designing scalable systems using Python, FastAPI, PostgreSQL, and Docker. Proficient in building machine learning pipelines, deploying models to production with MLflow and Streamlit. Strong advocate for clean code, CI/CD, and agile development.\n",
    "\n",
    "Skills:\n",
    "- Programming: Python, JavaScript, SQL, C++\n",
    "- ML/AI: Scikit-learn, PyTorch, Transformers, XGBoost, Hugging Face, LangChain\n",
    "- Web: FastAPI, Flask, Django, React, Next.js, Tailwind CSS\n",
    "- Databases: PostgreSQL, MySQL, MongoDB, Redis\n",
    "- DevOps: Docker, Kubernetes, GitHub Actions, Terraform, AWS, GCP\n",
    "- Tools: Jupyter, VS Code, Git, Postman, Slack, Notion\n",
    "- Soft Skills: Problem Solving, Teamwork, Communication, Leadership, Critical Thinking\n",
    "\n",
    "Experience:\n",
    "Senior Backend Engineer – DataStack AI (Remote)\n",
    "Aug 2021 – Present\n",
    "- Built a scalable FastAPI backend for a recommendation engine with PostgreSQL & Redis.\n",
    "- Containerized applications using Docker and deployed to GCP with Kubernetes.\n",
    "- Developed automated CI/CD pipelines using GitHub Actions.\n",
    "- Collaborated cross-functionally with frontend and ML teams using Agile.\n",
    "\n",
    "Machine Learning Engineer – TechNova Labs\n",
    "Jan 2019 – Jul 2021\n",
    "- Deployed NLP models for resume parsing and classification using Hugging Face Transformers.\n",
    "- Trained and tuned models with Scikit-learn, PyTorch, and MLflow.\n",
    "- Developed an interactive data dashboard with Streamlit and Plotly.\n",
    "\n",
    "Education:\n",
    "Bachelor of Computer Science, Cairo University, 2018\n",
    "GPA: 3.7 / 4.0\n",
    "\n",
    "Certifications:\n",
    "- AWS Certified Solutions Architect (2023–2026)\n",
    "- Google Cloud Professional ML Engineer\n",
    "- Microsoft Azure Fundamentals (AZ-900)\n",
    "- Deep Learning Specialization – Coursera (Andrew Ng)\n",
    "- ITIL v4 Foundation\n",
    "- Certified Kubernetes Administrator (CKA)\n",
    "\n",
    "Projects:\n",
    "AI Resume Analyzer\n",
    "- Built a Streamlit app that extracts and analyzes resume data using NLP and BERT NER.\n",
    "- Features: skill matching, email & phone extraction, certification parser.\n",
    "\n",
    "E-commerce REST API\n",
    "- Designed a secure REST API using Django REST Framework.\n",
    "- Integrated Stripe payments and user authentication with JWT.\n",
    "\n",
    "Languages:\n",
    "- Arabic (Native)\n",
    "- English (Fluent)\n",
    "- German (Intermediate)\n",
    "\n",
    "Interests:\n",
    "- Open source contributions\n",
    "- AI ethics & fairness\n",
    "- Playing chess & learning languages\n",
    "\"\"\"\n",
    "\n",
    "extract_languages(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "analyzer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
